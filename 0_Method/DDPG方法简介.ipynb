{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定性策略梯度\n",
    "\n",
    "这里研究确定性策略梯度方法，确定性策略是指对于某一个状态输出某一个动作，而不是动作的分布概率。与随机性策略相比，确定性策略可以解决动作空间维度很大的问题。但是它本身也存在这一些问题，例如由于输出动作是固定的，所以本身没有探索性，需要配合随机性策略进行学习。\n",
    "\n",
    "## DDPG\n",
    "\n",
    "* 1 DDPG是DPG的改进形式，他将其中的策略和评价函数都用深度神经网络来表示，DDPG或DPG是异策略的方法，行为策略为随机策略，评估策略为确定性策略。随机策略可以探索和产生多样的行为数据，确定性策略利用这些数据进行策略的改善。\n",
    "\n",
    "* 2 确定下策略梯度的计算公式如下：![](http://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ_%7B%5Cbeta%7D%5Cleft%28%5Cmu_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%7E%5Crho%5E%7B%5Cbeta%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%5Cnabla_aQ%5E%7B%5Cmu%7D%5Cleft%28s%2Ca%5Cright%29%7C_%7Ba%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%7D%5Cright%5D+%5C%5D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG可以称为是DQN的连续动作版，它同样利用了记忆池来打破数据时间相关性，利用Target网络来保持学习的稳定。它的网络结构如图：![](https://pic4.zhimg.com/50/v2-bb544a69bc8d28059d73694779cb05dc_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "它的训练过程可以理解为：\n",
    "\n",
    "* 从记忆中随机取出一部分样本\n",
    "\n",
    "* 根据Loss = r+q(s1,a1)-q(s2,a2)更新评价网络\n",
    "\n",
    "* 这一步之后，评价网络更接近策略网络真实的评价\n",
    "\n",
    "* 类比策略改进（每个状态选择最高的Q值，作为其输出动作）\n",
    "\n",
    "* 对于每个状态，策略网络目的是最大化Q值，这样选择的动作也是最优（理解为连续的按照梯度改进策略）\n",
    "\n",
    "* 计算得到的策略梯度，更新策略网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  参考\n",
    "\n",
    "[Actor-Crtic小结](https://zhuanlan.zhihu.com/p/29486661)\n",
    "\n",
    "[李宏毅A3C](https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/videos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
